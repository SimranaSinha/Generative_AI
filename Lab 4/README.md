# Lab 2: Prompting Strategies

## Overview

This lab focuses on exploring and comparing different prompting strategies used with large language models to improve reasoning quality, consistency, and correctness. The goal is to understand how prompt structure and guidance influence model outputs when applied to the same problem.

Rather than building a full application, this lab emphasizes prompt experimentation, analytical comparison, and thoughtful evaluation of model behavior.

---

## Objectives
	â€¢	Experiment with multiple prompting strategies on the same task
	â€¢	Understand how reasoning guidance affects model responses
	â€¢	Compare output quality across different prompting techniques
	â€¢	Analyze the strengths and limitations of each approach

---

## Project Structure

```
ðŸ“¦ Lab 4/
â”‚
â”œâ”€â”€ ðŸ“„ Simran Sinha_LangChainMemory.ipynb
â”‚
â”œâ”€â”€ ðŸ“Š beer_game_memory_comparison.xlsx
â”‚
â”œâ”€â”€ ðŸ“Š stock_market_memory_comparison.xlsx
â”‚
â””â”€â”€ ðŸ“˜ README.md   # Project documentation
```

---

## Tools & Technologies
- Jupyter Notebook
- Python
- OpenAI Chat Models
- Prompt engineering techniques

---

## Prompting Strategies Covered

The notebook demonstrates multiple prompting approaches applied to the same reasoning problem.
	â€¢	Zero-shot Prompting
  
Directly asks the model to solve the task without examples or additional guidance.
	â€¢	Few-shot Prompting
  
Provides one or more examples to guide the model toward the expected reasoning pattern.
	â€¢	Chain-of-Thought Prompting
  
Explicitly encourages step-by-step reasoning before arriving at a final answer.
	â€¢	Self-Consistency Strategy
  
Generates multiple reasoning paths and compares outputs to identify the most consistent result.
	â€¢	ReAct Style Prompting
  
Combines reasoning steps with intermediate actions to improve structured problem solving.

---

## Experiment Design
- The same logical problem is tested across different prompting strategies
- Model temperature values are varied to observe changes in creativity and determinism
- Outputs are evaluated for correctness, clarity, and depth of reasoning

---

## Observations & Analysis

Key observations include:
	â€¢	Structured prompts significantly improve reasoning accuracy
	â€¢	Chain-of-Thought prompting produces clearer and more interpretable responses
	â€¢	Higher temperature increases response diversity but can reduce consistency
	â€¢	Self-consistency improves reliability at the cost of additional computation

---

## Learning Outcomes

Through this lab, I gained practical experience with:
	â€¢	Designing effective prompts for reasoning-focused tasks
	â€¢	Understanding how temperature impacts model outputs
	â€¢	Comparing prompting strategies in a controlled experimental setup
	â€¢	Evaluating AI responses beyond surface-level correctness

---

## Notes

This lab is intended for academic learning and experimentation purposes. Model outputs should not be assumed to be correct without verification, especially in high-stakes or real-world applications.

