{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5208c400",
      "metadata": {
        "id": "5208c400"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 27px; letter-spacing: 0.5px;\">\n",
        "    ğŸ§  LangChain Lab 2: Prompt Templates &amp; Memory\n",
        "  </h2>\n",
        "  <p style=\"font-size: 17px; margin-bottom: 8px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Instructor:</span> Prof. Dehghani\n",
        "  </p>\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 12px;\">Lab Overview</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 14px;\">\n",
        "    In this lab, you'll enhance your interactions with LLMs by using <b>Prompt Templates</b> and <b>Memory</b> features in LangChain.<br>\n",
        "    You'll learn to create <b>structured prompts dynamically</b> and maintain <b>conversation history</b> across multiple turns.\n",
        "  </p>\n",
        "  <hr style=\"border: 1px solid #3f77d4; margin: 16px 0;\">\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">ğŸ¯ What You'll Learn</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 10px 22px; line-height: 1.8;\">\n",
        "    <li>ğŸ”¹ <b>Prompt Templates</b> â€“ Format inputs dynamically for LLMs.</li>\n",
        "    <li>ğŸ”¹ <b>Memory in LangChain</b> â€“ Maintain context in multi-turn conversations.</li>\n",
        "    <li>ğŸ”¹ <b>Hands-on exercises</b> â€“ Reinforce concepts with practical coding tasks.</li>\n",
        "  </ul>\n",
        "  <p style=\"font-size: 15.5px; margin-top: 8px;\">\n",
        "    By the end, you'll be able to structure prompts effectively and implement conversational memory in LangChain applications. ğŸš€\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##âš™ï¸ Install essential packages"
      ],
      "metadata": {
        "id": "8k2xSdP5989g"
      },
      "id": "8k2xSdP5989g"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b87a84d-d957-4a42-ccd6-a5a676d0f4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/496.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m491.5/496.3 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m496.3/496.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#âš™ï¸ Install essential packages for LangChain with OpenAI & Gemini support\n",
        "\n",
        "!pip -q install -U \\\n",
        "  langchain \\\n",
        "  langchain-core \\\n",
        "  langchain-community \\\n",
        "  langchain-openai \\\n",
        "  langchain-google-genai \\\n",
        "  langchain-classic \\\n",
        "  openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ğŸ”‘ Step 2: Set Up OpenAI API Key"
      ],
      "metadata": {
        "id": "C9B4mXQr93u8"
      },
      "id": "C9B4mXQr93u8"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927c5a53-9d01-4e19-99cf-2f95b8c7eb07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OpenAI API key loaded successfully!\n",
            "âœ… Google Gemini API key loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# âš™ï¸ Load API Keys from Colab Secrets\n",
        "# ==================================\n",
        "\n",
        "import os                                  # Used to set environment variables for API keys\n",
        "from google.colab import userdata          # To securely access stored secrets in Colab\n",
        "\n",
        "# Retrieve your stored secrets (API keys)\n",
        "OPENAI_API_KEY = userdata.get('openai.api_key')   # OpenAI API key for GPT models\n",
        "GEMINI_API_KEY = userdata.get('Gemini_API')   # Google Gemini API key for Gemini models\n",
        "\n",
        "# Set environment variables for the APIs and confirm success\n",
        "if OPENAI_API_KEY:\n",
        "  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY  # Set OpenAI key as environment variable\n",
        "  print(\"âœ… OpenAI API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"âŒ OpenAI API key not found. Please set 'OPENAI_API_KEY' in Colab secrets.\")\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    os.environ[\"Gemini_API\"] = GEMINI_API_KEY   # Set Gemini key as environment variable\n",
        "    print(\"âœ… Google Gemini API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"âŒ Google Gemini API key not found. Please set 'GEMINI_API_KEY' in Colab secrets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5047273f",
      "metadata": {
        "id": "5047273f"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 25px; letter-spacing: 0.5px;\">ğŸ“ Prompt Templates in LangChain</h2>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 8px;\">ğŸ”¹ What are Prompt Templates?</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 12px;\">\n",
        "    Prompt Templates let you <b>dynamically format prompts</b> by inserting variables, making interactions with LLMs more flexible and reusable.<br>\n",
        "    Instead of writing static text, you can use placeholders that are filled in with real values at runtime.\n",
        "  </p>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 8px;\">ğŸ”¹ Why Use Prompt Templates?</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.8;\">\n",
        "    <li>âœ… <b>Reusability</b> â€“ No need for repetitive prompts.</li>\n",
        "    <li>âœ… <b>Dynamic Inputs</b> â€“ Easily personalize prompts with new user data.</li>\n",
        "    <li>âœ… <b>Consistency</b> â€“ Keeps your prompt formatting structured and reliable.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"font-size: 17px; margin-bottom: 8px;\">ğŸ“Œ Example Usage</h3>\n",
        "  <div style=\"background: rgba(255,255,255,0.08); border-radius: 7px; padding: 11px 16px; margin-bottom: 6px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Static prompt:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of AI in healthcare?\"\n",
        "    </span>\n",
        "  </div>\n",
        "  <div style=\"background: rgba(255,255,255,0.08); border-radius: 7px; padding: 11px 16px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Dynamic prompt with a template:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of {technology} in {industry}?\"\n",
        "    </span><br>\n",
        "    <span style=\"font-size: 15px;\">If <b>{technology} = \"AI\"</b> and <b>{industry} = \"healthcare\"</b>, the prompt becomes:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of AI in healthcare?\"\n",
        "    </span>\n",
        "  </div>\n",
        "\n",
        "  <p style=\"font-size: 16px; margin-top: 14px;\">ğŸš€ Let's get started with the first example!</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7da3cc1a",
      "metadata": {
        "id": "7da3cc1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d154b9-6ac2-49c0-fb95-639ff007d70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ Generated Prompt: What are the benefits of Drones in SupplyChain in 1 sentence?\n",
            "ğŸ”¹ LLM Response: Drones in supply chain significantly reduce delivery times, increase efficiency and accuracy, lower operational costs, and enable access to remote or difficult-to-reach locations.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# ğŸ¯ Using Prompt Templates with OpenAI (GPT-4)\n",
        "# ==================================================\n",
        "\"\"\"\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI   # âœ… NEW import for ChatOpenAI\n",
        "\n",
        "# Step 1: Define a prompt template with variables\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"technology\", \"industry\"],\n",
        "    template=\"What are the benefits of {technology} in {industry} in 1 sentence?\"\n",
        ")\n",
        "\n",
        "# Step 2: Format the prompt with specific values\n",
        "formatted_prompt = prompt_template.format(technology=\"Drones\", industry=\"SupplyChain\")\n",
        "\n",
        "# Step 3: Initialize the OpenAI LLM (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "# Step 4: Generate the response\n",
        "response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "# Step 5: Display results\n",
        "print(\"ğŸ”¹ Generated Prompt:\", formatted_prompt)\n",
        "print(\"ğŸ”¹ LLM Response:\", response_ChatGPT.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# âœ‹ **Hands-On: Creating Dynamic Prompt Templates**\n",
        "# ==================================================\n",
        "\n",
        "# ğŸ“Œ **Task Instructions:**\n",
        "# 1ï¸âƒ£ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2ï¸âƒ£ Ensure the Prompt Template correctly replaces {topic} and {context}.\n",
        "# 3ï¸âƒ£ Run the code and verify GPT-4 generates a response.\n",
        "\n",
        "# âœ… Step 1: Define a Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"context\"],\n",
        "    template=\"How does {topic} impact {context} in a few words?\"\n",
        ")\n",
        "\n",
        "# âœ… Step 2: Format the prompt with actual values\n",
        "formatted_prompt = prompt_template.format(\n",
        "    topic=\"Machine Learning\",\n",
        "    context=\"business analytics\"\n",
        ")\n",
        "\n",
        "# âœ… Step 3: Generate a response using OpenAI (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")\n",
        "response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "# âœ… Step 4: Display results\n",
        "print(\"ğŸ”¹ Generated Prompt:\", formatted_prompt)\n",
        "print(\"ğŸ”¹ LLM Response:\", response_ChatGPT.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNUqPASY67El",
        "outputId": "0bac0e0a-34c7-43fe-a60b-f4c34eeb7efc"
      },
      "id": "eNUqPASY67El",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ Generated Prompt: How does Machine Learning impact business analytics in a few words?\n",
            "ğŸ”¹ LLM Response: Machine Learning enhances business analytics by automating complex data processing, improving accuracy of predictions, and enabling real-time analytics and decision making.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# âœ… Step 1: Define a Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"context\"],\n",
        "    template=\"How does {topic} impact {context} in a few words?\"\n",
        ")\n",
        "\n",
        "# âœ… Step 2: Format the prompt with actual values\n",
        "formatted_prompt = prompt_template.format(\n",
        "    topic=\"Machine Learning\",\n",
        "    context=\"business analytics\"\n",
        ")\n",
        "\n",
        "# âœ… Step 3: Generate a response using OpenAI (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")\n",
        "response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "# âœ… Step 4: Display results\n",
        "print(\"ğŸ”¹ **Generated Prompt:**\", formatted_prompt)\n",
        "print(\"ğŸ”¹ **LLM Response:**\", response_ChatGPT.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqX55MyG6bFP",
        "outputId": "27bdfea7-1910-447b-c3f0-b26ebbf7745e"
      },
      "id": "HqX55MyG6bFP",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ **Generated Prompt:** How does Machine Learning impact business analytics in a few words?\n",
            "ğŸ”¹ **LLM Response:** Machine Learning enhances business analytics by automating complex data analyses, providing accurate data-driven insights, enabling predictive modeling, improving decision making, and optimizing business processes and strategies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# ğŸ”„ Using Prompt Templates in a Loop\n",
        "# ==================================================\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Step 1ï¸âƒ£: Define a prompt template with variables\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"technology\", \"industry\"],\n",
        "    template=\"In one sentence, how does {technology} impact {industry} in 1 sentence?\"\n",
        ")\n",
        "\n",
        "# Step 2ï¸âƒ£: Initialize the OpenAI LLM (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "# Step 3ï¸âƒ£: Define input values for the loop\n",
        "input_data = [\n",
        "    {\"technology\": \"AI\", \"industry\": \"education\"},\n",
        "    {\"technology\": \"Blockchain\", \"industry\": \"finance\"},\n",
        "    {\"technology\": \"5G\", \"industry\": \"telecommunications\"},\n",
        "]\n",
        "\n",
        "# Step 4ï¸âƒ£: Loop through inputs, format the prompt, and generate a response\n",
        "for data in input_data:\n",
        "    formatted_prompt = prompt_template.format(**data)\n",
        "    response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "    # Step 5ï¸âƒ£: Display results in a clear, modern format\n",
        "    print(f\"ğŸ”¹ Prompt: {formatted_prompt}\")\n",
        "    print(f\"ğŸ’¡ Response: {response_ChatGPT.content}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "MQE2mEke-6_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fabdfb1-cc5f-4d5a-da3d-f429a0a39951"
      },
      "id": "MQE2mEke-6_w",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ Prompt: In one sentence, how does AI impact education in 1 sentence?\n",
            "ğŸ’¡ Response: AI impacts education by individualizing learning and teaching processes, providing personalized support and feedback, automifying administrative tasks, and making educational resources more accessible and interactive.\n",
            "------------------------------------------------------------\n",
            "ğŸ”¹ Prompt: In one sentence, how does Blockchain impact finance in 1 sentence?\n",
            "ğŸ’¡ Response: Blockchain revolutionizes the finance industry by providing a decentralized, secure system for transactions, eliminating the need for intermediaries, increasing efficiency, and reducing fraud.\n",
            "------------------------------------------------------------\n",
            "ğŸ”¹ Prompt: In one sentence, how does 5G impact telecommunications in 1 sentence?\n",
            "ğŸ’¡ Response: 5G revolutionizes telecommunications by providing significantly faster data transmission speeds, higher capacity, lower latency, and the ability to connect more devices simultaneously, transforming digital experiences and enabling the growth of the Internet of Things (IoT).\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #1a386e 0%, #377ce8 100%); color: white; padding: 24px 26px 16px 26px; border-radius: 12px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h3 style=\"margin-top: 0; font-size: 21px;\">ğŸ”— Wrapping Up: Why Prompt Templates Matter</h3>\n",
        "  <p style=\"font-size: 16px;\">\n",
        "    Just as you wouldn't rewrite a whole menu for every customer in a coffee shop, you don't need to create a new prompt for every question you ask an LLM.\n",
        "    <br><br>\n",
        "    <b>Prompt templates</b> give you a reusable, flexible, and structured way to interact with language modelsâ€”making your code cleaner, your queries more consistent, and your applications easier to scale.\n",
        "    <br><br>\n",
        "    Whether you're building a chatbot, automating business tasks, or analyzing data, prompt templates are an essential tool in your GenAI toolkit!\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "B9jXv_aeFukI"
      },
      "id": "B9jXv_aeFukI"
    },
    {
      "cell_type": "markdown",
      "id": "3c9bdf18",
      "metadata": {
        "id": "3c9bdf18"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 27px; letter-spacing: 0.5px;\">\n",
        "    ğŸ§  Understanding Memory in LangChain\n",
        "  </h2>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">ğŸ”¹ What is Memory in LangChain?</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 13px;\">\n",
        "    By default, LLMs <b>do not remember past interactions</b>.<br>\n",
        "    LangChain <b>Memory</b> allows an AI model to <b>retain context</b> across multiple turns, enabling more natural, conversational interactions.\n",
        "  </p>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">ğŸ”¹ Why Use Memory?</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.8;\">\n",
        "    <li>âœ… <b>Maintains conversation history</b> â€“ AI can recall previous exchanges.</li>\n",
        "    <li>âœ… <b>Improves response coherence</b> â€“ Reduces redundant user re-explanations.</li>\n",
        "    <li>âœ… <b>Essential for chatbots &amp; agents</b> â€“ Allows multi-turn dialogue without loss of context.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">ğŸ”¹ Types of Memory in LangChain</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.7;\">\n",
        "    <li>1ï¸âƒ£ <b>ConversationBufferMemory</b> â€“ Stores messages in a buffer (basic memory).</li>\n",
        "    <li>2ï¸âƒ£ <b>ConversationSummaryMemory</b> â€“ Summarizes past interactions instead of storing all messages.</li>\n",
        "    <li>3ï¸âƒ£ <b>ConversationBufferWindowMemory</b> â€“ Retains only the last N interactions for efficiency.</li>\n",
        "    <li>4ï¸âƒ£ <b>Vector-based Memory</b> â€“ Uses embeddings for advanced retrieval of past conversations.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"font-size: 17px; margin-bottom: 8px;\">ğŸš€ What We'll Do in This Lab</h3>\n",
        "  <p style=\"font-size: 16px;\">\n",
        "    Weâ€™ll start with <b>ConversationBufferMemory</b>, which allows an LLM to <b>recall past messages</b> and interact in a more natural, memory-enhanced way.<br>\n",
        "    <br>\n",
        "    <span style=\"color: #a5d8ff;\">Note:</span> When using a conversation chain with memory, youâ€™ll use the <b><code>predict()</code></b> method instead of <code>invoke()</code>. This lets the AI maintain and use context across multiple turns.<br>\n",
        "    <br>\n",
        "    Let's get started! ğŸ‘‡\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "225bb6e6",
      "metadata": {
        "id": "225bb6e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d833da-1b02-4f74-fb69-1ce4616242e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== WITHOUT MEMORY (Stateless LLM) ==========\n",
            "ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: Our anniversary is October 5th. Please remember that!\n",
            "ğŸ¤– AnniversaryBot: Sure, I will remember that your anniversary is on October 5th. However, as an AI, I don't have the capability to remind you of your anniversary or any upcoming events. You may want to set a reminder on your smart device to ensure that you don't forget the special date.\n",
            "ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: When is our anniversary?\n",
            "ğŸ¤– AnniversaryBot: As an AI, I don't own personal relationships or events hence I don't have an anniversary.\n",
            "\n",
            "========== WITH MEMORY ==========\n",
            "ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: Our anniversary is October 5th. Please remember that!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3389284213.py:31: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/tmp/ipython-input-3389284213.py:33: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation_with_mem = ConversationChain(llm=llm_with_mem, memory=memory)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– AnniversaryBot: Sure, I will remember that October 5th is your anniversary! Just to confirm, an anniversary typically is a special day that marks a significant event in the past, such as a wedding. Is that correct in your case?\n",
            "ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: When is our anniversary?\n",
            "ğŸ¤– AnniversaryBot: Your anniversary is on October 5th, as you previously mentioned.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# ğŸ’¬ğŸ§  Memory Matters: AnniversaryBot Demo (Stateless vs. Memory)\n",
        "# ==================================================\n",
        "#\n",
        "# This demo shows how LangChain's memory feature allows an AI assistant to remember\n",
        "# detailsâ€”using the example of a user telling the bot their anniversary date.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.chains import ConversationChain\n",
        "\n",
        "# -------------------------------\n",
        "# 1ï¸âƒ£ Version WITHOUT Memory (Stateless)\n",
        "# -------------------------------\n",
        "llm_stateless = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "print(\"\\n========== WITHOUT MEMORY (Stateless LLM) ==========\")\n",
        "print(\"ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: Our anniversary is October 5th. Please remember that!\")\n",
        "response1 = llm_stateless.invoke(\"Our anniversary is October 5th. Please remember that!\")\n",
        "print(\"ğŸ¤– AnniversaryBot:\", response1.content)\n",
        "\n",
        "print(\"ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: When is our anniversary?\")\n",
        "response2 = llm_stateless.invoke(\"When is our anniversary?\")\n",
        "print(\"ğŸ¤– AnniversaryBot:\", response2.content)\n",
        "\n",
        "# -------------------------------\n",
        "# 2ï¸âƒ£ Version WITH Memory\n",
        "# -------------------------------\n",
        "memory = ConversationBufferMemory()\n",
        "llm_with_mem = ChatOpenAI(model_name=\"gpt-4\")\n",
        "conversation_with_mem = ConversationChain(llm=llm_with_mem, memory=memory)\n",
        "\n",
        "print(\"\\n========== WITH MEMORY ==========\")\n",
        "print(\"ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: Our anniversary is October 5th. Please remember that!\")\n",
        "response3 = conversation_with_mem.predict(input=\"Our anniversary is October 5th. Please remember that!\")\n",
        "print(\"ğŸ¤– AnniversaryBot:\", response3)\n",
        "\n",
        "print(\"ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: When is our anniversary?\")\n",
        "response4 = conversation_with_mem.predict(input=\"When is our anniversary?\")\n",
        "print(\"ğŸ¤– AnniversaryBot:\", response4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9dc20167",
      "metadata": {
        "id": "9dc20167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1246fa2d-632f-43be-dcee-28747aef68ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¬ Retailer: Last week, the customer demand was 200 units. What should I order this week?\n",
            "ğŸ¤– ChatGPT: Without sufficient data like trends, seasonality, or changes in conditions, I cannot provide a specific figure for how many units to order this week. Could you provide more information? For example, an increase or decrease in demand, special events or holidays, inventory levels, supplier lead times, etc. these factors can greatly help in making a more informed decision.\n",
            "\n",
            "ğŸ’¬ Retailer: If demand increases by 10%, how many units should I prepare for next week?\n",
            "ğŸ¤– ChatGPT: If demand increases by 10%, that means you should expect a demand of 220 units for next week, as 10% of 200 is 20 and added to the original demand, this equals 220. However, don't forget to consider other variables such as inventory in hand and lead time to replenish stocks.\n",
            "\n",
            "ğŸ’¬ Retailer: What was the demand I mentioned last week?\n",
            "ğŸ¤– ChatGPT: You mentioned last week that the customer demand was 200 units.\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# âœ‹ Hands-On: Using Memory with OpenAI (Beer Game - Supply Chain Predictions)\n",
        "# ==================================================\n",
        "\n",
        "# ğŸ“Œ Task Instructions:\n",
        "# 1ï¸âƒ£ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2ï¸âƒ£ Ensure the AI remembers previous demand data and predicts future order quantities.\n",
        "# 3ï¸âƒ£ Run the code and check if ChatGPT maintains context for supply chain decisions.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.chains import ConversationChain\n",
        "\n",
        "# âœ… Step 1: Initialize Memory\n",
        "memory = ConversationBufferMemory()  # Initialize the correct memory class\n",
        "\n",
        "# âœ… Step 2: Initialize ChatGPT with Memory\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")  # Initialize ChatGPT model\n",
        "conversation = ConversationChain(llm=llm_ChatGPT, memory=memory)  # Attach memory to conversation\n",
        "\n",
        "# âœ… Step 3: Run Multiple Interactions\n",
        "print(\"\\nğŸ’¬ Retailer: Last week, the customer demand was 200 units. What should I order this week?\")\n",
        "response_ChatGPT = conversation.predict(\n",
        "    input=\"Last week, the customer demand was 200 units. What should I order this week?\"\n",
        ")\n",
        "print(\"ğŸ¤– ChatGPT:\", response_ChatGPT)\n",
        "\n",
        "print(\"\\nğŸ’¬ Retailer: If demand increases by 10%, how many units should I prepare for next week?\")\n",
        "response_ChatGPT = conversation.predict(\n",
        "    input=\"If demand increases by 10%, how many units should I prepare for next week?\"\n",
        ")\n",
        "print(\"ğŸ¤– ChatGPT:\", response_ChatGPT)\n",
        "\n",
        "print(\"\\nğŸ’¬ Retailer: What was the demand I mentioned last week?\")\n",
        "response_ChatGPT = conversation.predict(\n",
        "    input=\"What was the demand I mentioned last week?\"\n",
        ")\n",
        "print(\"ğŸ¤– ChatGPT:\", response_ChatGPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using 'Summarized Conversation' Example\n"
      ],
      "metadata": {
        "id": "3ihDbUh4E5Sn"
      },
      "id": "3ihDbUh4E5Sn"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ğŸ¤ **Using Memory in LangChain: Job Interview Prep**\n",
        "# ==================================================\n",
        "#\n",
        "# This script simulates a job interview practice session.\n",
        "# It uses ConversationSummaryMemory to retain key points from previous exchanges\n",
        "# rather than storing the full conversation history.\n",
        "\n",
        "# âœ… Import required classes\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationSummaryMemory  # Summarized conversation memory\n",
        "from langchain_classic.chains import ConversationChain\n",
        "\n",
        "# âœ… Step 1: Initialize Memory\n",
        "# This memory will maintain a **summarized** version of the conversation.\n",
        "memory = ConversationSummaryMemory(llm=ChatOpenAI(model_name=\"gpt-4\"))\n",
        "\n",
        "# âœ… Step 2: Initialize ChatGPT with Memory\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")  # Using GPT-4 model\n",
        "\n",
        "# âœ… Step 3: Initialize Conversation Chain\n",
        "# The model will summarize key details from the job interview practice.\n",
        "conversation = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "# âœ… Step 4: Conduct the Interview Simulation\n",
        "\n",
        "print(\"\\nğŸ’¬ User: Can you ask me a common interview question?\")\n",
        "response = conversation.predict(input=\"Can you ask me a common interview question?\")\n",
        "print(\"ğŸ¤– ChatGPT:\", response)\n",
        "\n",
        "# âœ… Check memory after first interaction\n",
        "print(\"\\nğŸ“œ Memory Summary After 1st Question:\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nğŸ’¬ User: My biggest strength is adaptability and problem-solving.\")\n",
        "response = conversation.predict(input=\"My biggest strength is adaptability and problem-solving.\")\n",
        "print(\"ğŸ¤– ChatGPT:\", response)\n",
        "\n",
        "# âœ… Check memory after user shares strength\n",
        "print(\"\\nğŸ“œ Memory Summary After Strength Response:\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nğŸ’¬ User: My biggest weakness is that I sometimes overthink decisions.\")\n",
        "response = conversation.predict(input=\"My biggest weakness is that I sometimes overthink decisions.\")\n",
        "print(\"ğŸ¤– ChatGPT:\", response)\n",
        "\n",
        "# âœ… Check memory after user shares weakness\n",
        "print(\"\\nğŸ“œ Memory Summary After Weakness Response:\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nğŸ’¬ User: Can you summarize what we discussed so far?\")\n",
        "response = conversation.predict(input=\"Can you summarize what we discussed so far?\")\n",
        "print(\"ğŸ¤– ChatGPT:\", response)\n",
        "\n",
        "# âœ… Final Memory Check\n",
        "print(\"\\nğŸ“œ Final Memory Summary:\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n"
      ],
      "metadata": {
        "id": "PGhKAmB-E3vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4a322b-ddd5-470d-87b1-97f7a2d3caa0"
      },
      "id": "PGhKAmB-E3vm",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¬ User: Can you ask me a common interview question?\n",
            "ğŸ¤– ChatGPT: Of course! Here's a common one that incorporates both personal insight and professional development: \"Can you describe a situation in which you faced a challenge and how you handled it?\"\n",
            "\n",
            "ğŸ“œ Memory Summary After 1st Question:\n",
            "The human asked the AI to propose a common interview question. The AI responded with a question about personal insight and professional development, asking the human to describe a situation in which they faced a challenge and how they handled it.\n",
            "\n",
            "ğŸ’¬ User: My biggest strength is adaptability and problem-solving.\n",
            "ğŸ¤– ChatGPT: That's great to hear! Adaptability and problem-solving are indeed critical skills in today's rapidly changing world. Can you provide a specific instance where these strengths were the most useful? An account of a situation can help better understand how you apply these strengths in real-life scenarios.\n",
            "\n",
            "ğŸ“œ Memory Summary After Strength Response:\n",
            "The human asked the AI to propose a common interview question, to which the AI asked the human to describe a challenging situation and how they handled it. The human responded by indicating that their biggest strengths are adaptability and problem-solving. The AI applauded the human for these valuable skills while furthering the discussion by asking them to provide a specific instance where these strengths were most useful.\n",
            "\n",
            "ğŸ’¬ User: My biggest weakness is that I sometimes overthink decisions.\n",
            "ğŸ¤– ChatGPT: That's a fair self-assessment. Overthinking can sometimes lead to paralysis by analysis, which may delay decision-making. However, on the flip side, it also indicates a careful and thoughtful approach. Can you share an instance where overthinking affected your decision-making process either positively or negatively?\n",
            "\n",
            "ğŸ“œ Memory Summary After Weakness Response:\n",
            "The human asked the AI to propose a common interview question, and the AI asked about a challenging situation that the human handled. The human expressed that their strengths are adaptability and problem-solving, and the AI praised these skills, asking for a specific example. The human admitted to overthinking decisions as their biggest weakness, and the AI acknowledged this as a common issue that can lead to careful thought but also paralysis by analysis. The AI asked for an instance where overthinking influenced the human's decision-making.\n",
            "\n",
            "ğŸ’¬ User: Can you summarize what we discussed so far?\n",
            "ğŸ¤– ChatGPT: Absolutely. Initially, I pitched an interview question about handling a challenging situation. You responded by stating your strengths, namely adaptability and problem-solving. I then asked for a specific instance where you utilized these strengths. When asked about your weakness, you confessed to overthinking decisions. Acknowledging this, I explained that overthinking can result in thorough consideration though it might lead to paralysis by analysis sometimes. Following this, I requested an example when overthinking factored into your decision-making.\n",
            "\n",
            "ğŸ“œ Final Memory Summary:\n",
            "The human and the AI discussed a typical interview question scenario, in which the AI asked about a challenging situation the human has handled. The human identified their strengths as adaptability and problem-solving, leading the AI to request a specific example of these skills in action. The human confessed their biggest weakness is overthinking decisions, which the AI recognizes as a common issue, causing both careful consideration and potential analysis paralysis. The AI then asked for an example of a situation where overthinking impacted the human's decision-making. In response to a request from the human, the AI summarized these various points of the conversation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ğŸº **LangChain Beer Game: Comparing Memory Types\n",
        "# ==================================================\n",
        "#\n",
        "# This script simulates a Beer Game ordering process over 6 weeks.\n",
        "# It uses:\n",
        "# 1ï¸âƒ£ ConversationBufferMemory (Tracks full history)\n",
        "# 2ï¸âƒ£ ConversationBufferWindowMemory (Tracks only last 3 orders)\n",
        "#\n",
        "# The AI predicts the next order quantity based on past interactions.\n",
        "\n",
        "# âœ… Import required libraries\n",
        "import pandas as pd\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# âœ… Step 1: Initialize Memory Types\n",
        "buffer_memory = ConversationBufferMemory(return_messages=True)  # Stores entire history\n",
        "window_memory = ConversationBufferWindowMemory(k=3, return_messages=True)  # Stores last 3 interactions\n",
        "\n",
        "# âœ… Step 2: Initialize Chat Model (NEW!)\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# âœ… Step 3: Define a Prompt Template\n",
        "beer_game_template = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"\n",
        "    You are managing a supply chain for a beer distribution system.\n",
        "    Orders fluctuate at first but stabilize later.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on past orders, what should be the next order quantity?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# âœ… Step 4: Define Processing Pipelines\n",
        "buffer_chain = beer_game_template | llm\n",
        "window_chain = beer_game_template | llm\n",
        "\n",
        "# âœ… Step 5: Define Order Fluctuations (First 3 weeks volatile, last 3 weeks stable)\n",
        "weekly_orders = [20, 50, 10, 25, 30, 30]  # Example fluctuations\n",
        "\n",
        "# Store results for comparison\n",
        "buffer_memory_log = []\n",
        "window_memory_log = []\n",
        "buffer_predictions = []\n",
        "window_predictions = []\n",
        "\n",
        "# âœ… Step 6: Run the Simulation\n",
        "for week in range(1, len(weekly_orders) + 1):\n",
        "    prev_orders = \", \".join(map(str, weekly_orders[:week]))  # Orders seen so far\n",
        "    context = f\"Week {week}: The previous orders were {prev_orders}.\"\n",
        "\n",
        "    # Store input in memory\n",
        "    buffer_memory.save_context({\"context\": context}, {\"response\": \"\"})\n",
        "    window_memory.save_context({\"context\": context}, {\"response\": \"\"})\n",
        "\n",
        "    # Get AI predictions using RunnableSequence\n",
        "    buffer_prediction = buffer_chain.invoke({\"context\": context})\n",
        "    window_prediction = window_chain.invoke({\"context\": context})\n",
        "\n",
        "    # Retrieve memory states\n",
        "    buffer_memory_summary = buffer_memory.load_memory_variables({})[\"history\"]\n",
        "    window_memory_summary = window_memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Store memory states and predictions\n",
        "    buffer_memory_log.append(buffer_memory_summary)\n",
        "    window_memory_log.append(window_memory_summary)\n",
        "    buffer_predictions.append(buffer_prediction.content)\n",
        "    window_predictions.append(window_prediction.content)\n",
        "\n",
        "# âœ… Step 7: Display Results in a Table\n",
        "df = pd.DataFrame({\n",
        "    \"Week\": list(range(1, len(weekly_orders) + 1)),\n",
        "    \"Actual Orders\": weekly_orders,\n",
        "    \"Buffer Memory (Stores All)\": buffer_memory_log,\n",
        "    \"Window Memory (Last 3 Turns)\": window_memory_log,\n",
        "    \"Buffer Memory Prediction\": buffer_predictions,\n",
        "    \"Window Memory Prediction\": window_predictions\n",
        "})\n"
      ],
      "metadata": {
        "id": "uFTXdRyVFUNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c88d41-4b4c-4d1b-cd62-1d00972d92f4"
      },
      "id": "uFTXdRyVFUNC",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-90158075.py:20: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  window_memory = ConversationBufferWindowMemory(k=3, return_messages=True)  # Stores last 3 interactions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "dRm1ZzStWDLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "a6508000-856b-42c9-d3ef-a53ff1e460e5"
      },
      "id": "dRm1ZzStWDLa",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Week  Actual Orders                         Buffer Memory (Stores All)  \\\n",
              "0     1             20  [content='Week 1: The previous orders were 20....   \n",
              "1     2             50  [content='Week 1: The previous orders were 20....   \n",
              "2     3             10  [content='Week 1: The previous orders were 20....   \n",
              "3     4             25  [content='Week 1: The previous orders were 20....   \n",
              "4     5             30  [content='Week 1: The previous orders were 20....   \n",
              "5     6             30  [content='Week 1: The previous orders were 20....   \n",
              "\n",
              "                        Window Memory (Last 3 Turns)  \\\n",
              "0  [content='Week 1: The previous orders were 20....   \n",
              "1  [content='Week 1: The previous orders were 20....   \n",
              "2  [content='Week 1: The previous orders were 20....   \n",
              "3  [content='Week 2: The previous orders were 20,...   \n",
              "4  [content='Week 3: The previous orders were 20,...   \n",
              "5  [content='Week 4: The previous orders were 20,...   \n",
              "\n",
              "                            Buffer Memory Prediction  \\\n",
              "0  Without additional past order data, determinin...   \n",
              "1  This would depend on the pattern and the reaso...   \n",
              "2  The average of previous orders (20, 50, 10) ca...   \n",
              "3  The data provided is quite limited to make an ...   \n",
              "4  The orders seem to fluctuate quite a bit, but ...   \n",
              "5  Based on the trend, the next order quantity sh...   \n",
              "\n",
              "                            Window Memory Prediction  \n",
              "0  As there's not enough data provided about past...  \n",
              "1  The information provided isn't sufficient to a...  \n",
              "2  Calculating the average of the previous orders...  \n",
              "3  The trend does not show a consistent pattern, ...  \n",
              "4  Determining the next order quantity would requ...  \n",
              "5  The next order quantity should be based on rec...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73f58899-02af-4ca8-954b-1b570ecd3e25\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Week</th>\n",
              "      <th>Actual Orders</th>\n",
              "      <th>Buffer Memory (Stores All)</th>\n",
              "      <th>Window Memory (Last 3 Turns)</th>\n",
              "      <th>Buffer Memory Prediction</th>\n",
              "      <th>Window Memory Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>Without additional past order data, determinin...</td>\n",
              "      <td>As there's not enough data provided about past...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>50</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>This would depend on the pattern and the reaso...</td>\n",
              "      <td>The information provided isn't sufficient to a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>The average of previous orders (20, 50, 10) ca...</td>\n",
              "      <td>Calculating the average of the previous orders...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 2: The previous orders were 20,...</td>\n",
              "      <td>The data provided is quite limited to make an ...</td>\n",
              "      <td>The trend does not show a consistent pattern, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>30</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 3: The previous orders were 20,...</td>\n",
              "      <td>The orders seem to fluctuate quite a bit, but ...</td>\n",
              "      <td>Determining the next order quantity would requ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 4: The previous orders were 20,...</td>\n",
              "      <td>Based on the trend, the next order quantity sh...</td>\n",
              "      <td>The next order quantity should be based on rec...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73f58899-02af-4ca8-954b-1b570ecd3e25')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-73f58899-02af-4ca8-954b-1b570ecd3e25 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-73f58899-02af-4ca8-954b-1b570ecd3e25');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_5e3d8ce7-6510-4a7a-a7e9-b388c2ebfcba\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5e3d8ce7-6510-4a7a-a7e9-b388c2ebfcba button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Week\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual Orders\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 10,\n        \"max\": 50,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          50,\n          30,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buffer Memory (Stores All)\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Window Memory (Last 3 Turns)\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buffer Memory Prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Without additional past order data, determining the next order quantity can be challenging. However, if orders have been stabilizing at around 20 per week, it would be reasonable to order 20 units for the next week as well. Future adjustments can then be made based on customer behavior and demand changes. A more accurate estimate could be given if more historical data were available.\",\n          \"This would depend on the pattern and the reason behind the fluctuation. If a trend or a pattern can be identified or business intelligence related to market demand/supply shared, a more precise prediction could be made. However, based on just two provided data points, it is hard to accurately project the next order quantity. A simple average of the previous order quantities (which is 35) could be considered as a rough estimate. A more comprehensive prediction method would involve forecasting techniques using multiple past data points, considering seasonal variations, trends, etc.\",\n          \"Based on the trend, the next order quantity should be around 30. The order quantities are indeed fluctuating but the last two weeks seem to have stabilized at 30. It's advisable to keep an eye on the trend and adjust accordingly in case the demand increases or decreases.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Window Memory Prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"As there's not enough data provided about past orders trends and no specific pattern mentioned, it would be safe to match the previous order quantity. Therefore, the next order quantity should also be 20. However, it's good practice to regularly review order quantities as more data become available over time.\",\n          \"The information provided isn't sufficient to accurately predict the next order quantity. More data points or a specific trend or pattern is needed to make a reliable prediction.\",\n          \"The next order quantity should be based on recent trends among previous orders. Given the fluctuations, it would be best to calculate the average. \\n\\nBy calculating the average of the last six weeks, the total sum is:\\n\\n20 + 50 + 10 + 25 + 30 + 30 = 165.\\n\\nNow dividing by 6 (number of weeks), the average (rounded to nearest whole number) is:\\n\\n165 / 6 = approximately 28.\\n\\nSo, the next order quantity should be around 28. However, as the two recent orders have been of 30, you should consider ordering 30 (or a bit more) to avoid any potential out-of-stock situation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Save the table to an Excel file\n",
        "df.to_excel(\"beer_game_memory_comparison.xlsx\", index=False)\n",
        "\n",
        "# âœ… Print confirmation message\n",
        "print(\"Data saved to 'beer_game_memory_comparison.xlsx'\")\n"
      ],
      "metadata": {
        "id": "tlLyE0DYB6Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6a7c9f-f5b4-4d29-ed0a-04af5074bf83"
      },
      "id": "tlLyE0DYB6Qt",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to 'beer_game_memory_comparison.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“Œ **Assignment: AI Stock Market Trend Prediction with Memory**\n",
        "\n",
        "## **Objective**\n",
        "In this assignment, you will use AI to predict stock market trends based on historical stock prices. You will compare how different memory types affect AI's ability to track and predict future trends.\n",
        "\n",
        "## **Tasks**\n",
        "1. **Initialize memory types** (`ConversationBufferMemory` and `ConversationBufferWindowMemory`).\n",
        "2. **Define the AI model** (GPT-4 or another suitable model).\n",
        "3. **Complete the prompt template** to guide AI predictions.\n",
        "4. **Process stock price data** and use memory to store past trends.\n",
        "5. **Retrieve and analyze stored memory** after each step.\n",
        "6. **Invoke the AI model correctly** to generate predictions.\n",
        "7. **Save results to an Excel file** for analysis.\n",
        "\n",
        "## **Expected Outcome**\n",
        "You will observe how AI predictions change when it has full history vs. limited memory. This will help you understand the impact of memory in AI-based forecasting.\n",
        "\n",
        "ğŸš€ **Complete the placeholders and run the script to generate insights!** ğŸš€\n"
      ],
      "metadata": {
        "id": "r9Y518a4MeVJ"
      },
      "id": "r9Y518a4MeVJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# âœ‹ **Hands-On: Creating Dynamic Prompt Templates**\n",
        "# ğŸ“ˆ **AI Assignment: Stock Market Trend Prediction with Memory**\n",
        "# ==================================================\n",
        "#\n",
        "# ğŸ”¹ In this assignment, you will use AI to predict stock market trends.\n",
        "# ğŸ”¹ You will compare how different memory types affect AI's ability to track stock price movements.\n",
        "# ğŸ”¹ Complete the placeholders (----) to make the script functional.\n",
        "#\n",
        "# ğŸ“Œ **Your Tasks:**\n",
        "# 1ï¸âƒ£ Initialize the correct memory types.\n",
        "# 2ï¸âƒ£ Define the AI model.\n",
        "# 3ï¸âƒ£ Complete the template prompt.\n",
        "# 4ï¸âƒ£ Use memory correctly when processing stock data.\n",
        "# 5ï¸âƒ£ Ensure correct invocation of AI for predictions.\n",
        "# 6ï¸âƒ£ Retrieve and analyze stored memory.\n",
        "# 7ï¸âƒ£ Save results in an Excel file.\n",
        "\n",
        "# âœ… Import required libraries\n",
        "import pandas as pd\n",
        "from langchain_classic.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "# âœ… Step 1: Initialize Memory Types\n",
        "buffer_memory = ConversationBufferMemory()                 # Stores full stock history\n",
        "window_memory = ConversationBufferWindowMemory(k=3)        # Stores last 3 turns\n",
        "\n",
        "# âœ… Step 2: Initialize Chat Model\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)        # Define the AI model\n",
        "\n",
        "# âœ… Step 3: Define a Prompt Template\n",
        "stock_prediction_template = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI financial analyst predicting stock market trends.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on this stock price history, what will be the next trend (Up, Down, or Stable)?\n",
        "    Reply with: Trend = Up/Down/Stable, and 1 short reason.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# âœ… Step 4: Define Processing Pipelines\n",
        "buffer_chain = stock_prediction_template | llm\n",
        "window_chain = stock_prediction_template | llm\n",
        "\n",
        "# âœ… Step 5: Define Stock Price Data (Fluctuations in the first weeks, then stabilizing)\n",
        "stock_prices = [120, 125, 110, 130, 128, 129]\n",
        "\n",
        "# Store results for comparison\n",
        "buffer_memory_log = []\n",
        "window_memory_log = []\n",
        "buffer_predictions = []\n",
        "window_predictions = []\n",
        "\n",
        "# âœ… Step 6: Run the Prediction Simulation\n",
        "for week in range(1, len(stock_prices) + 1):\n",
        "    prev_prices = \", \".join(map(str, stock_prices[:week]))\n",
        "    context = f\"Week {week}: The previous stock prices were {prev_prices}.\"\n",
        "\n",
        "    # Store input in memory\n",
        "    buffer_memory.save_context({\"input\": context}, {\"output\": \"\"})\n",
        "    window_memory.save_context({\"input\": context}, {\"output\": \"\"})\n",
        "\n",
        "    # Retrieve memory states (this is what will differ!)\n",
        "    buffer_memory_summary = buffer_memory.load_memory_variables({})[\"history\"]\n",
        "    window_memory_summary = window_memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Get AI predictions USING memory summaries as the context\n",
        "    buffer_prediction = buffer_chain.invoke({\"context\": buffer_memory_summary})\n",
        "    window_prediction = window_chain.invoke({\"context\": window_memory_summary})\n",
        "\n",
        "    # Store memory states and predictions\n",
        "    buffer_memory_log.append(buffer_memory_summary)\n",
        "    window_memory_log.append(window_memory_summary)\n",
        "    buffer_predictions.append(buffer_prediction.content)\n",
        "    window_predictions.append(window_prediction.content)\n",
        "\n",
        "# âœ… Step 7: Save Results in an Excel File\n",
        "df = pd.DataFrame({\n",
        "    \"Week\": list(range(1, len(stock_prices) + 1)),\n",
        "    \"Stock Price\": stock_prices,\n",
        "    \"Buffer Memory (Stores All)\": buffer_memory_log,\n",
        "    \"Window Memory (Last 3 Turns)\": window_memory_log,\n",
        "    \"Buffer Memory Prediction\": buffer_predictions,\n",
        "    \"Window Memory Prediction\": window_predictions\n",
        "})\n",
        "\n",
        "df.to_excel(\"stock_market_memory_comparison.xlsx\", index=False)\n",
        "\n",
        "# âœ… Print confirmation message\n",
        "print(\"Assignment completed! Data saved to 'stock_market_memory_comparison.xlsx'\")\n"
      ],
      "metadata": {
        "id": "TGapDKV0HJWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2690af61-52f3-4a42-ea80-50566cbb7cf6"
      },
      "id": "TGapDKV0HJWg",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment completed! Data saved to 'stock_market_memory_comparison.xlsx'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}